\documentclass{article}
\usepackage{times}
\begin{document}


\title{EAS 591D Finite Element Methods for Viscous Flow}
\author{Scott D. King}
\date{Spring 2006}
\maketitle

\pagestyle{myheadings}
\markright{Finite Element Methods for Viscous Flow}
\setcounter{section}{1}
\setcounter{page}{40}
\section{Finite Element Method}

Now we begin our introduction to the finite element method.  To start with, I
will consider the Possion Equation, because it avoids a complication that we
will come to in a few weeks, while illustrating all the important points about
second order differential equations.   Recall Possion's equation
\begin{equation}
\nabla^2 u = f.
\end{equation}

I am going to follow a similar form to Hughes (The Finite Element Method)
Chapter~1 verses 1-4.  Lets think today in 1-D.  There are two forms we can
write the equation, the strong and the weak form.  You are used to seeing the
strong form, but not used to seeing the weak form.   The finite element method,
is cast in the weak form.   In elasticity, for example, the weak form comes
from a variational principal, such as the principal of virtual displacements. 
For viscous flow, there is also a variational form, but we will not go into
that.

\subsection{The Strong Form}

The strong form can be formally stated: given $f: \bar \Omega \rightarrow \Re $
and constants $g$ and $h$ find $u: \bar \Omega \rightarrow \Re$
\begin{eqnarray}
u,_{xx} ~+~ f & = & 0 \\
u(1) & = & g \\
u,_x (0) + h & = & 0 
\end{eqnarray}
This choice of initial conditions allows us to examine both kinds of boundary
conditions.  The solution is trivial, but that does not matter.  For
completeness, it is
\begin{equation}
u(x) = g + (1-x) h + \int_x^1 \left ( \int_0^y f(z) dz \right ) dy
\end{equation}


\subsection{The Weak Form}

To state the weak form formally, we define a set of trial solutions
\begin{equation}
\delta ~=~ \{ u~|~u ~\in H^1 , ~~u(1)=g \}
\end{equation}
and define a set of weighting functions
\begin{equation}
\varphi ~=~ \{ w~|~w ~\in H^1 , ~~w(1)=0 \}
\end{equation}
Given $f, g,$ and $h$ as before. Find $u \in \delta$ such that for all
$w \in \varphi$
\begin{equation}
\int_0 ^1 w,_x u,_x dx ~=~ \int_0 ^1 w f dx + w(0) h 
\end{equation}
In mechanics, the $w$'s are the virtual displacements, and the weak form is the
variational equation.

\subsection{Equivalence of Stong and Weak forms}

We need to define a concept of a square integrable function, sometimes writen as
$H^1$-functions.   A function is square integrable on
$[0,1]$ if
\begin{equation}
\int_0^1 (u,_x)^2 dx < \infty.
\end{equation}

We can show that the strong and weak forms of the equation are equivalent. 
Since both possess unique solutions (I will leave that for pure mathematicians
to prove) the strong and weak solutions are one in the same.

We begin with $u$ which we will assume is a solution to the strong form.  Thus
we can write
\begin{equation}
0 = - \int_0 ^1 w \{ u,_{xx} + f \} dx
\end{equation}
for $w \in \varphi$.  Using integration by parts:
\begin{equation}
0 ~=~  \int_0 ^1 w,_x u,_x dx  -  \int_0 ^1 w\, f dx  - w\, u,_x |^{^1}_{_0}  
\end{equation} 
so
\begin{equation}
\int_0 ^1 w,_x u,_x dx ~=~ \int_0 ^1 w\, f\, dx + w(0) h.
\end{equation}
This is the statement of the weak form of the problem.  Furthermore, since $u$
is a solution of the strong form $u(1) = g$ and is in $\delta$.  

We can work the problem backwards, starting with the weak form, using
integration by parts and the definition of square integrability to prove the
weak form is equivalent to the strong form.  We begin with $v$ a solution to
the weak form
\begin{equation}
\int_0 ^1 w,_x v,_x dx ~=~ \int_0 ^1 w\, f\, dx + w(0) h.
\end{equation}
We will use integration by parts and $w(1)  = 0$ to get
\begin{equation} 
0 = -\int_0 ^1 w \{ v,_{xx} + f \} dx + w(0) \{ v,_x (0) + h \}.
\label{eq:intbyparts}
\end{equation}
To prove $v$ is a solution to the strong form, $ v,_{xx} + f = 0$ on
$\Omega$ and $v,_x (0) + h = 0$.   The first can be accomplished by choosing
\begin{equation}
w = \phi \left ( v,_{xx} + f \right )
\end{equation}
where $\phi$ is smooth, greater than zero inside the domain and zero at the
boundaries.   A good choice might be $\phi = x \left ( 1 -x \right )$.  
Subsitiuting our choice of $w$ into~\ref{eq:intbyparts} we get
\begin{equation} 
0 = \int _0 ^ 1 \phi \left ( v,_{xx} + f \right )^2 dx + 0
\end{equation}
Because the squared term must be greater than or equal to zero, and  $\phi >
0$ in the domain, the only possible solution is that $v,_{xx} + f = 0$ within
the domain $\Omega$.  Using that fact, we can substitute back into
Equation~\ref{eq:intbyparts} to get $v,_x (0) + h = 0$.

\subsection{Galerkin's Approximation}

Now we have a start on the finite element method.  I want to continue to follow
Hughes; however his notation becomes quite difficult to keep up with.  Now,
lets begin to think about putting a solution on the computer.   Because we will
have a finite approximation, related to how fine we space our grid, our
solution will only approximate the real solution.   Following Hughes' notation,
the solution on the grid will be denoted as $u^h$ where $h$ is some measure of
the spacing at the grid.   Then, 
\begin{equation}
\int_0 ^1 w^h,_x u^h,_x dx ~=~ \int_0 ^1 w^h\, f^h\, dx + w^h(0) h.
\label{eq:weak}
\end{equation}
approximates our exact solution $u$.

On a computer, we don't have a continuous solution.  We have a solution at
descrete points.  We need to approximate the solution between the points (in
order to integrate over the function).    We will do this with {\bf shape
functions} as they are usually called in the finite element language.  Hughes
uses $N_A  ~~~A=1, 2, \cdots , n$ to denote the shape functions.   You can also
think of these as basis functions or interpolation functions.  We require $ N_A
(1) = 0, A=1,2, \cdots, n$.   In order to specify our boundary condition, we
need another shape function which has the property
\begin{equation}
N_{n+1} (1) = 1.
\end{equation}
Then, $g^h$ is given by,
\begin{equation}
g^h = g N_{n+1} 
\end{equation}
and thus,
\begin{equation} 
g^h (1) = g.
\end{equation}
With these definitions, we can write our solution $u^h$ as
\begin{equation}
u^h = \sum_{A=1}^{n} \, d_A \, N_A + g N_{n+1}
\end{equation}
where the $d_A$'s are unknown constants to be solved for.

At this point, I don't want you to become discouraged, because we will make the
shape functions more concrete.   I want you to see how general this is, because
in principle there is a great deal of flexibility in how we choose the shape
functions.

We have not said anything more about this function $w^h$ and how we are going
to choose it.    If our shape functions form a basis set for the grid,  then we
can represent {\bf any} function as a sum of the basis functions times some
arbitrary coefficients $c_i$, 
\begin{equation}
w^h = \sum_{A=1}^{n} \, c_A \, N_A = c_1 \, N_1 ~+~ c_2 \, N_2 ~+~ \cdots ~+~
c_n \, N_n \label{eq:whapprox}
\end{equation}
If you don't remember this part of your mathematics background think of Fourier
series.  Any function one-dimensional function can be represented as an
infinite series of sines and cosines times some unique set of coefficients.  
The shape functions form a similar kind of basis set.

Notice that because we required that $ N_A (1) = 0, A=1,2, \cdots, n$,
Equation~\ref{eq:whapprox} satisfies the requirement that $w^h (1) = 0$, as
necessary.   

Using our definitions of the $w^h$'s and our approximation for $u^h$, we can
get the messy expression for Equation~\ref{eq:weak}
\begin{displaymath}
\int_0 ^1 \left ( \frac{\partial}{\partial x} 
\left ( \sum_{A=1}^{n} \, c_A \, N_A \right )
\frac{\partial}{\partial x}  
\left ( \sum_{B=1}^{n} \, d_B \, N_B + g N_{n+1} \right )  \right ) dx ~=~ 
\end{displaymath}
\begin{equation}
\int_0 ^1 \sum_{A=1}^{n} \, c_A \, N_A  \, f^h\, dx + 
\sum_{A=1}^{n} \, c_A N_A (0) h.
\end{equation}
By rearranging, we can write
\begin{equation}
\sum_{A=1}^{n} G_A c_A = 0
\end{equation}
where
\begin{displaymath}
G_A ~=~ \int_0 ^1 \left ( \frac{\partial N_A}{\partial x} \right )
\left ( \sum_{B=1}^{n} \, d_B \, \frac{\partial N_B}{\partial x}  \right ) dx 
\end{displaymath}
\begin{equation}
- \int_0 ^1  N_A  \, f^h\, dx - N_A (0) h + 
\int_0 ^1  \frac{\partial N_A}{\partial x}  \frac{\partial N_{n+1}}{\partial x}
\, g\, dx
\end{equation}
Now I use the fact that the shape functions are basis functions, so 
$N_A \times N_B$ is zero except when $A = B$.  We could equally well use the
fact that the $c_A$'s are arbitrary.   Both of these force us to conclude that
each $G_A$ must be identically zero and we get
\clearpage
\begin{displaymath} 
\sum_{B=1}^{n} \, \left ( \int_0 ^1 \frac{\partial N_A}{\partial x} 
\frac{\partial N_B}{\partial x} \, dx \right ) \, d_B =
\end{displaymath}
\begin{equation}
\int_0 ^1  N_A  \, f^h\, dx + N_A (0) h - 
g\, \int_0 ^1  \frac{\partial N_A}{\partial x}
\frac{\partial N_{n+1}}{\partial x} \, dx \label{eq:feform}
\end{equation}
Everything in Equation~\ref{eq:feform} is known except the $d_B$'s.   This
constitutes a system of $n$ equations and $n$ unknowns.  We can think of the
left hand side as a matrix, $K_{AB}$ whose entries are 
\begin{equation}
\int_0 ^1 \frac{\partial N_A}{\partial x} \frac{\partial N_B}{\partial x} \, dx
\label{eq:Kelements}
\end{equation}
We can write
\begin{equation} 
\sum_{B=1}^{n} \, K_{AB} d_B = F_A, ~~~~~ A = 1, 2, \cdots, n
\end{equation}
or as a matrix equation
\begin{equation}
\left [ K \right ] \, \{d\} = \{f\}
\end{equation}
where
\begin{equation} 
[k] = \left[ 
\begin{array}{cccc}
K_{11} & K_{12} & \cdots & K_{1n} \\ 
K_{21} & K_{22} & \cdots & K_{2n} \\
\vdots & \vdots &        & \vdots \\
K_{n1} & K_{n2} & \cdots & K_{nn} 
\end{array} \right ] \label{Kmatrix} 
\end{equation}
By tradition, $\left [ K \right ]$ is the stiffness matrix, $\{f\}$ is the force
vector, and $\{d\}$ is the displacement vector.  When the problem under
consideration pertains to a mechanical system, this makes the most sense, but
even in heat conduction problems, or fluid flow problems, the terminology
is still (often) retained.

\subsection{Some Simple Problems}

Lets look at a problem with one degree of freedom ($n\, =\, 1$).   It will
illustrate some important concepts.   If $n\, =\, 1$, then $w^h = c_1 \, N_1$
and $u^h \, = \, d_1 N_1 ~+~ g N_2$.  The only unknown is $d_1$.   The shape
functions must satisfy $N_1(1) ~=~ 0$ and $N_2(1) ~=~ 1$.   Lets choose 
\begin{eqnarray}
N_1(x) ~=~ 1 \, - \, x \label{eq:n1} \\
N_2(x) ~=~ x \label{eq:n2}
\end{eqnarray}
clearly $N_1$ and $N_2$ satisfy our required conditions.   Because we have only
one degree of freedom, the matrix collapses as follows:
\begin{eqnarray}
[K] ~=~ K_{11} \\
\{f\} ~=~ F_1 \\
\{d\} ~=~ d_1
\end{eqnarray}
Using Equation~\ref{eq:Kelements}, and substituting Equations~\ref{eq:n1}
and~\ref{eq:n2} we find
\begin{equation}
K_{11} ~=~ \int _0 ^1 N_{1,x} N_{1,x} \, dx ~=~ \int _0 ^1 \{ -1\} \{-1\} \, dx
~=~ 1
\end{equation}
and
\begin{eqnarray}
F_1 &~=~& \int _0 ^1 N_1(x) \, f(x) \, dx ~+~ N_1(0) \, h ~-~  g \, \int _0 ^1
N_{1,x} \, N_{2,x} \, dx \nonumber \\
    &~=~& \int _0 ^1 \left (1\, -\, x \right ) f(x) \, dx ~+~ h ~-~  
g \,  \int _0 ^1 \{ -1 \} \{ 1 \} \, dx \nonumber \\
    &~=~& \int _0 ^1 \left (1\, -\, x \right ) f(x) \, dx ~+~ h ~+~ g
\end{eqnarray} 
so
\begin{equation}
d_1 ~=~ K_{11}^{-1} F_1 ~=~ F_1
\end{equation}
So if we substitute $d_1, N_1,$ and, $ N_2$ back into our expression for $u^h$,
we get
\begin{eqnarray}
u^h \, = \, d_1 N_1 ~+~ g N_2 \\
u^h (x) \, = \, \left ( \int _0 ^1 \left (1\, -\, y \right ) f(y) \, dy ~+~ h ~+~
g \right ) \left ( 1 ~-~ x \right ) ~+~ g \, x \\
\end{eqnarray}
which is the finite element approximate solution for $u(x)$.

It is helpful to compare this to the exact solution from last class,
\begin{equation} 
u(x) ~=~ g ~+~ (1 - x) \, h ~+~ \int_x^1 \left ( \int_0^y f(z) dz \right ) dy
\end{equation}

Lets look as several specific cases:

\noindent
{\bf Case 1:} $f(x) = 0 $.  In this case
\begin{eqnarray}
u(x)       &=& g ~+~ (1 - x) \, h \\
u^h (x) \, &=& (h + g) \, ( 1 ~-~ x ) ~+~ g \, x \nonumber \\
           &=&  g ~+~ (1 - x) \, h 
\end{eqnarray}
In this case, $u(x) = u^h(x)$.

\noindent {\bf Case 2:} $f(x) = p ~$ where $p$ is a constant.  In this case
\begin{eqnarray} 
u(x)       &=& g ~+~ (1 - x) \, h ~+~ \frac{p(1-x^2)}{2}\\ 
u^h (x) \, &=& (h + g) \, ( 1 ~-~ x ) ~+~ g \, x + \frac{p(1-x)}{2}\nonumber \\
           &=&  g ~+~ (1 - x) \, h + \frac{p(1-x)}{2}
\end{eqnarray}
Notice that $u(x) = u^h(x)$ at $x=0$ and $x=1$.   Also note that 
\begin{equation}
u_{,x}( \frac{1}{2} ) = u^h_{,x}( \frac{1}{2} ).
\end{equation}

\noindent {\bf Case 3:} $f(x) = p \, x ~$ where $p$ is a constant.  In this
case
\begin{eqnarray}  
u(x)       &=& g ~+~ (1 - x) \, h ~+~ \frac{p(1-x^3)}{6}\\ 
u^h (x) \, &=& (h + g) \, ( 1 ~-~ x ) ~+~ g \, x + \frac{p(1-x)}{6}\nonumber \\
           &=&  g ~+~ (1 - x) \, h + \frac{p(1-x)}{6}
\end{eqnarray} 
Notice that $u(x) = u^h(x)$ at $x=0$ and $x=1$.   Also note that 
\begin{equation} 
u_{,x}( \frac{1}{\sqrt{3}} ) = u^h_{,x}( \frac{1}{\sqrt{3}} ).
\end{equation}
There are four points worth remembering from this exercise
\begin{enumerate}
\item The homogeneous equation ($f = 0$) was exact.

\item With $f \ne 0$, $u^h = u$ at $x=0$ and $x=1$.

\item With $f \ne 0$, there was at least one point where $u_{,x^h} = u{,_x}$

\item The highest order polynomial in $x$ in the finite element solution $u^h$ is
never greater than the highest order polynomial in the shape function.

\end{enumerate}

\subsection{Shape Functions}

At this point, I will narrow the focus to deal with specifically the elements
in my code, ConMan.   It is possible to think very general shape functions,
but in practice, people use triangles or quadralaterals (in 2-D).   We will
extend our finite element formulation to a 2-D equation next time. 
In terms of the level of approximation, there are also a lot of possibilities. 
We will stick to the simplest form, bi-linear elements, but you should be aware
that higher order elements (bi-quadratic or bi-cubic spline elements) are also
popular with some people.   I am condensing a lot of very useful material from
Chapter 3 of Hughes' book into one lecture.   If you want to see more complete
derivations, proofs of convergence, etc., of how to go about using higher order
elements, look at Hughes book, Chapter 3.

Lets start by thinking of a rectangle that is 2a by 2b in length centered at
(0,0).  There are two properties we would like the shape functions to have
\begin{eqnarray}
\sum_{A=1} ^4 N_A(X,Y) & = & 1 \label{eq:normalize} \\
\sum_{A=1} ^4 N_A(X,Y) \, X_A & = & X \label{eq:interpx} \\
\sum_{A=1} ^4 N_A(X,Y) \, Y_A & = & Y \label{eq:interpy}
\end{eqnarray}
Equation~\ref{eq:normalize} says that they are normalized, so that they sum to
one (everywhere on X,Y).  Equations ~\ref{eq:interpx} and~\ref{eq:interpy}
state that the shape functions are also interpolation functions.   Without
doing a lot of derivation, I will claim that for the rectangle described
above,
\begin{eqnarray}
N_1 & = & \frac{(a-x)(b-y)}{4ab}  \\
N_2 & = & \frac{(a+x)(b-y)}{4ab}  \\
N_3 & = & \frac{(a+x)(b+y)}{4ab}  \\
N_4 & = & \frac{(a-x)(b+y)}{4ab}  
\end{eqnarray}
these shape functions satisfy the conditions in Equation~\ref{eq:normalize} and
Equations ~\ref{eq:interpx} and~\ref{eq:interpy}.  A good exercise would be to
show this is true.

\begin{quote} 
{\bf Problem 7} Show that the shape functions defined
above satisfy Equation~\ref{eq:normalize} and Equations
~\ref{eq:interpx} and~\ref{eq:interpy}.
\end{quote} 

Notice that by convention, I start numbering my element nodes in the lower left
hand corner and work counter-clockwise.   {\em This is a very important point. 
It is followed through out all my finite element codes.}  There is no magic
reason, you just have to choose a starting place.

In ConMan, as in Hughes, we further choose to normalize this by setting $a = 1$
and $b=1$.  This choice gives us an element whose area is 1, which is a
convenient way to think about things.  (This is because we left the factor of
1/4 in the denominator).  In my code, to make one less set of computations, I in
effect set $a=0.5$ and $b=0.5$ so that the denominator goes to 1.

Notice it is pretty easy to take derivatives of these shape functions
\begin{eqnarray} 
N_{1,x} & = & \frac{-(1-y)}{4}  \\ 
N_{2,x} & = & \frac{(1-y)}{4}  \\ 
N_{3,x} & = & \frac{(1+y)}{4}  \\ 
N_{4,x} & = & \frac{-(1+y)}{4}  
\end{eqnarray}
\begin{eqnarray}  
N_{1,y} & = & \frac{-(1-x)}{4}  \\  
N_{2,y} & = & \frac{-(1+x)}{4}  \\  
N_{3,y} & = & \frac{(1+x)}{4}  \\  
N_{4,y} & = & \frac{(1-x)}{4}  
\end{eqnarray}
What do we do if we want to solve a problem on a domain that is not convenient
to split into a grid of 1 by 1 unit elements?   We use an important principle
of mathematics, the jacobian of the transformation
\begin{equation}
K_{11} ~=~ \int _A ^B N_{1,x} N_{1,x} \, dx ~=~ \int _0 ^1 N_{1,x} N_{1,x} \,
J dx
\end{equation}
where $J$ is the Jacobian of the transformation.  This is a very powerful
point.   When we are thinking of solving a regular Cartesian domain, this just
corresponds to a stretching or a shrinking (notice we set $a=b=1$ above.  
However, if we are thinking about a cylindrical geometry, for example, we can
use the Jacobian of the transformation between the geometries.   Lets look at two
examples:

Converting an element 0.05 by 0.10 centered at (0.1,0.2) to the `parent element'
centered at (0,0).     Hughes also uses $\xi, \eta$ for the $X,Y$ coordinate
pair in the `parent element'   So we could write
\begin{eqnarray}
x & = & 0.1 + {0.05} \xi + 0.0 \eta\\
y & = & 0.2 + {0.0} \xi + {0.10} \eta 
\end{eqnarray}
or in matrix form we could write
\begin{equation} 
\left \{ \begin{array}{c} x \\ y \end{array} \right \}  = 
\left [ \begin{array}{cc} 
0.05 & 0.0 \\  
0.0 & 0.10
\end{array} \right ] \left \{ \begin{array}{c}\xi \\ \eta \end{array} \right \}
+  \left \{ \begin{array}{c} 0.1 \\ 0.2 \end{array} \right \} 
\end{equation}
Where $[J]$ is the Jacobian of the transformation.  If the transformation were
from an arbitratry shaped quadralateral to the parent element, then the off
diagonal terms will not be zero.   It is easy enough to show that
\begin{equation}
\int _{x_1} ^{x_2} \int _{y_1} ^{y_2}  f(x,y) \, dx\, dy = 
\int _{-1} ^1 \int _{-1} ^1 f( \xi , \eta ) \, {\rm det} [J] \, d\xi \, d\eta
\end{equation}
It turns out, and it is also easy to show, that ${\rm det} [J]$ is the ratio of
the areas when going from one rectangle to another (in fact any Cartesian to
Cartesian transformation).

{\bf Advanced Topic:}  Now suppose we want to map a cylindrical domain to our
`parent element.'  We can use the same principle in this case:
\begin{eqnarray} 
x &=& r \cos \theta = \cos \theta \, \xi - r \sin \theta \, \eta \\ 
y &=& r \sin \theta = \sin \theta \, \xi + r \cos \theta \, \eta 
\end{eqnarray}
so
\begin{equation}
{\rm det} [J_{geometry}] ~=~ r \cos^2 \theta  + r \sin^2 \theta = r.
\end{equation}
of we would get
\begin{equation}
\int _{r_1} ^{r_2} \int _{\theta_1} ^{\theta_2}  f(r\cos\theta, r\sin\theta) \,
r \, dr\, d\theta = 
\int _{-1} ^1 \int _{-1} ^1 f( \xi , \eta ) \, {\rm det} [J_{area}] \, d\xi \,
d\eta
\end{equation}

\subsubsection{Gauss Quadrature}

An amazing fact, that makes the idea of finite elements easy and powerful is
Gauss Quadrature.   Gauss Quadrature is a way to turn an integral into a
summation.   Lets begin with a 1-D example, f(x) = c
\begin{equation}
\int _{-1} ^1 c \, dx = c x | _{-1} ^1 = 2 c 
\end{equation}
Gauss noted that for any linear function 
\begin{equation}
\int _{-1} ^{1} f(x) \, dx =  2.0 \times f( 0 ) = 2 c
\end{equation}
For a linear function, f(x) = a x + b
\begin{equation}
\int _{-1} ^{1} f(x) \, dx = f(\frac{-1}{\sqrt{3}}) + f(\frac{-1}{\sqrt{3}})
\end{equation}
The direct way,
\begin{equation}
\int _{-1} ^{1} (ax + b) \, dx = (\frac{ax^2}{2} + bx) | _{-1} ^1 = 
\frac{a}{2} + b - (\frac{a}{2} -b) = 2b
\end{equation}
Gauss' way
\begin{equation}
\int _{-1} ^{1} (ax + b) \, dx = a \frac{-1}{\sqrt{3}} + b + a \frac{1}{\sqrt{3}}
+ b = 2b
\end{equation}
It turns out that $\frac{-1}{\sqrt{3}}, \frac{1}{\sqrt{3}}$ are exact for a
linear equation, but from what I showed, so would any $-x, x$ combination, but
what Gauss showed was more powerful, that if the function is of higher order,
the $ \frac{-1}{\sqrt{3}}, \frac{1}{\sqrt{3}}$ choice is the best approximation
you can make with only two terms.   If we go to three terms, the choice would
be $ -\sqrt{\frac{3}{5}} , 0 , \sqrt{\frac{3}{5}}$.

To integrate a 2-D Cartesian region, like our parent element, it turns out that
2 by 2 quadrature, or the four points 
\begin{eqnarray}   
\xi =  \frac{-1}{\sqrt{3}} & ~~~ & \eta =  \frac{-1}{\sqrt{3}} \\
\xi =  \frac{ 1}{\sqrt{3}} & ~~~ & \eta =  \frac{-1}{\sqrt{3}} \\
\xi =  \frac{ 1}{\sqrt{3}} & ~~~ & \eta =  \frac{ 1}{\sqrt{3}} \\
\xi =  \frac{-1}{\sqrt{3}} & ~~~ & \eta =  \frac{ 1}{\sqrt{3}} 
\end{eqnarray}
are sufficient to exactly integrate our bilinear shape functions over the
{-1,-1} to {1,1} domain.

At this point, it would be worth talking about the code ConMan for a minute.
The shape functions are generated in ConMan in the routine {\bf genshp} for
GENerate SHape functions Parent domain.  If you look at the routine you will
find the first part of it is pretty easy to follow from the discussion in
todays lecture.   Some of the second part is a little tricky in its details,
but generally it is also pretty easy to follow.   The subroutine local will 
be discussed in detail next lecture.  The subroutine genshg will follow this
one.    Some constants to keep in mind (ConMan was written very generally,
so we tried not to hard-wire element sizes into it.
\begin{enumerate}
\item numnp = total NUMber of Nodal Points

\item numel = total NUMber of ELements

\item nsd = Number of Spacial Dimensions (2 for a 2-D problem).

\item nen = Number of Element Nodes (4 for the bi-linear quad).

\item shl = SHape function Local (array corresponding to equations 54-57).

\item shdx = SHape function Derivative X (array of x derivs)

\item shdy = SHape function Derivative Y (array of y derivs)
\end{enumerate}

\tt
\begin{verbatim}
        subroutine genshp (  x,  xl   ,ien   , locblk , shl  , shdx ,
     &                    shdy,    det, nel  , nblk   , elval)
c
c----------------------------------------------------------------------
c
c This program generates the shape functions for bi-linear,and
c  calculates the min element dimension per node 
c
c input: 
c  x      (nsd,numnp)      : coordinates 
c  xl     (lvec,nsd,nen)   : local coordinates 
c  ien    (numel,nen)      : ien array 
c  nel                     : number of elements 
c  nblk                    : number of element blocks 
c 
c output: 
c 
c        shl  (nen,nipt) 
c        shdx (nel,nen,nipt) 
c        shdy (nel,nen,nipt) 
c        det  (nel,nipt) 
c        eval (nel,6) 
c 
c Note: the last five arrays are setup with element as the first index.
c 
c
      implicit double precision (a-h,o-z) 
c 
c.... deactivate above card(s) for single precision operation 
c
      include 'common.h' 
c
      dimension x(nsd,1)   , ien(nel),  locblk(2,1),
     &          shl(nen,1) , shdx(1) ,  shdy(1) ,
     &          det(1)     , xl(lvec,nsd,nen)   , elval(nel,1) 
c
      common /temp1 / sa(4),ta(4),sg(5),tg(5),shldx(4,5),shldy(4,5) 
c 
c.... set up parameters 
c
      sa(1) = -pt5
      sa(2) =  pt5
      sa(3) =  pt5
      sa(4) = -pt5 
c
      ta(1) = -pt5
      ta(2) = -pt5
      ta(3) =  pt5
      ta(4) =  pt5 
c
      guass = one / sqrt(three) 
c

      sg(1) = -one * guass
      sg(2) =  one * guass
      sg(3) =  one * guass
      sg(4) = -one * guass
      sg(5) = zero * guass 
c
      tg(1) = -one * guass
      tg(2) = -one * guass
      tg(3) =  one * guass
      tg(4) =  one * guass
      tg(5) = zero * guass 
c
      call clear (shdx, nel*nen*nipt)
      call clear (shdy, nel*nen*nipt) 
c 
c   generate the generic shape functions
c
      do 100 j = 1,5
      do 100 i = 1,4
      shl(i,j)   = (pt5 + sa(i) * sg(j) ) * (pt5 + ta(i) * tg(j) )
      shldx(i,j) = sa(i)*(pt5 + ta(i) * tg(j) )
      shldy(i,j) = ta(i)*(pt5 + sa(i) * sg(j) ) 
100   continue
c
      do 1000 iblk = 1, nblk 
c 
c.... set up the pointers 
c
        iel  = locblk(1,iblk)
        nenl = locblk(2,iblk)
        nvec = locblk(1,iblk+1) - iel
        call local (ien(iel), x, xl, nsd, nenl, 'localize') 
c 
c.... call the global shape function 
c
        call genshg (shl      , shldx   , shldy ,  shdx,
     &               shdy     , det     , xl    , iel   ) 
c 
c.... calculate min element dimension per node 
c
      do 500 iv = 1 ,nvec
      ivel = iel + iv - 1
      exse1 = pt5*( xl(iv,1,2) + xl(iv,1,3) - xl(iv,1,4) - xl(iv,1,1) )
      exse2 = pt5*( xl(iv,2,2) + xl(iv,2,3) - xl(iv,2,4) - xl(iv,2,1) )
      eeta1 = pt5*( xl(iv,1,3) + xl(iv,1,4) - xl(iv,1,1) - xl(iv,1,2) )
      eeta2 = pt5*( xl(iv,2,3) + xl(iv,2,4) - xl(iv,2,1) - xl(iv,2,2) )
      hxse = sqrt(exse1*exse1 + exse2*exse2)
      heta = sqrt(eeta1*eeta1 + eeta2*eeta2)
      elval(ivel ,1) = exse1/hxse
      elval(ivel ,2) = exse2/hxse
      elval(ivel ,3) = eeta1/heta
      elval(ivel ,4) = eeta2/heta
      elval(ivel ,5) = hxse
      elval(ivel ,6) = heta 
500   continue
c 
1000      continue
c 
c.... return 
c
      return
      end
\end{verbatim}

\rm
Next we look at {\bf genshg} for GENerate SHape functions Global.   This is
where the parent elements  get mapped to the `real' elements.   Basically it
calculates a lot of determinents.

\tt
\begin{verbatim}
      subroutine  genshg (shl  , shldx , shldy ,  shdx  , shdy  ,
     &                    det  , xl  ,   iel  ) 
c
c---------------------------------------------------------------------- 
c 
c This program generates the globalshape functions for bi-linear, 
c 
c input: 
c  xl     (lvec,nsd,nen)      : local coordinates 
c  shldx  (nen ,nipt )        : local dx 
c  shldy  (nen ,nipt )        : local dy 
c  shl    (nen ,nipt )        : local shape functions 
c c output: 
c 
c        shdx (nel,nen,nipt) 
c        shdy (nel,nen,nipt) 
c        det  (nel,nipt) 
c 
c Note: the last four arrays are setup with element as the first index. 
c       This should facilitate vectorization. 
c
c 
c---------------------------------------------------------------------- 
c 
c
      implicit double precision (a-h,o-z) 
c 
c.... deactivate above card(s) for single precision operation 
c
      include 'common.h' 
c
      dimension shl(nen,1), shldx(nen,1), shldy(nen,1), xl(lvec,nsd,1)
      dimension shdx(numel,nen,1) ,shdy(numel,nen,1) ,det(numel,1) 
c
      common /tempx/ xs(lvec,2,2) , temp(lvec) 
c 
c.... loop over all the integration points 
c
      do 1000 l = 1 , nipt 
c 
c     find jocabian 
c
      do 100 iv = 1 , nvec
         xs(iv,1,1) = xl(iv,1,1) * shldx(1,l) + xl(iv,1,2) * shldx(2,l)
     &              + xl(iv,1,3) * shldx(3,l) + xl(iv,1,4) * shldx(4,l) 
c
         xs(iv,1,2) = xl(iv,2,1) * shldx(1,l) + xl(iv,2,2) * shldx(2,l)
     &              + xl(iv,2,3) * shldx(3,l) + xl(iv,2,4) * shldx(4,l) 
c
         xs(iv,2,1) = xl(iv,1,1) * shldy(1,l) + xl(iv,1,2) * shldy(2,l)
     &              + xl(iv,1,3) * shldy(3,l) + xl(iv,1,4) * shldy(4,l) 
c
         xs(iv,2,2) = xl(iv,2,1) * shldy(1,l) + xl(iv,2,2) * shldy(2,l)
     &              + xl(iv,2,3) * shldy(3,l) + xl(iv,2,4) * shldy(4,l) 
100   continue 
c 
c..... calculate the inverse jacobian 
c
      do 200 iv = 1 , nvec
         ivel = iv + iel - 1
         det(ivel,l) = xs(iv,1,1) * xs(iv,2,2) - xs(iv,1,2) * xs(iv,2,1) 
200   continue 
c 
c.... check for zero determine 
c
      do 300 iv = 1 , nvec
         ivel=iv+iel-1
         if ( det(ivel,l) .le. zero )
     &      call error (' genshg  ','det-jacb',iel) 
300   continue 
c 
c.... continue inverse calculation 
c.... and find derivative with respect to global axes 
c
      do 400 iv = 1 , nvec
        ivel=iv+iel-1
        temp(iv) = det(ivel,l)
        temp(iv) = one/temp(iv)
        shdx(ivel,1,l) = temp(iv) * ( xs(iv,2,2) * shldx(1,l)
     &                 - xs(iv,1,2) * shldy(1,l) )
        shdy(ivel,1,l) = temp(iv) * (-xs(iv,2,1) * shldx(1,l)
     &                 + xs(iv,1,1) * shldy(1,l) ) 
c
        shdx(ivel,2,l) = temp(iv) * ( xs(iv,2,2) * shldx(2,l)
     &                 - xs(iv,1,2) * shldy(2,l) )
        shdy(ivel,2,l) = temp(iv) * (-xs(iv,2,1) * shldx(2,l)
     &                 + xs(iv,1,1) * shldy(2,l) ) 
c
        shdx(ivel,3,l) = temp(iv) * ( xs(iv,2,2) * shldx(3,l)
     &                 - xs(iv,1,2) * shldy(3,l) )
        shdy(ivel,3,l) = temp(iv) * (-xs(iv,2,1) * shldx(3,l)
     &                 + xs(iv,1,1) * shldy(3,l) ) 
c
        shdx(ivel,4,l) = temp(iv) * ( xs(iv,2,2) * shldx(4,l)
     &                 - xs(iv,1,2) * shldy(4,l) )
        shdy(ivel,4,l) = temp(iv) * (-xs(iv,2,1) * shldx(4,l)
     &                 + xs(iv,1,1) * shldy(4,l) ) 
400   continue 
c 
c.... end of integration point loop 
c 
1000  continue 
c 
c.... return 
c
      return
      end
\end{verbatim}
\rm
\clearpage

There are two strange data structures and loops that appear in the subroutines
above.   The first is,
\tt\begin{verbatim}
      do 1000 iblk = 1, nblk  
c  
c.... set up the pointers  
c
        iel  = locblk(1,iblk)
        nenl = locblk(2,iblk)
        nvec = locblk(1,iblk+1) - iel
\rm
\end{verbatim}
and the other is loops like
\tt\begin{verbatim}
      do 200 iv = 1 , nvec
         ivel = iv + iel - 1
         det(ivel,l) = xs(iv,1,1) * xs(iv,2,2) - xs(iv,1,2) * xs(iv,2,1)  
200   continue 
\end{verbatim}\rm
These are done because on a vector computer you can not be sure that assemble
operations will be handled correctly unless you are very careful.   This
involves a little thinking into how a vector computer works.    I have removed all of this
coding from the test code you will be working with, so don't get hung up on it.
It is a relic from the days of Cray computers.

\clearpage

\subsection{The Element Point of View}

Transforming between the element and global points of view is done
with the data structure called the IEN array for Element to Node 
transformation.    The IEN array takes an element number and a local node
number and it's value is the global node number.   It is easiest to look at
some examples:

Consider a 3 element by 3 element grid.  I will number my elements and node
starting in the lower left hand corner, and working up fastest.   

\begin{verbatim}
For element 3:
        ien (3, 1) = 3 
        ien (3, 2) = 7
        ien (3, 3) = 8
        ien (3, 4) = 4
For element 5:
        ien (5, 1) = 6
        ien (5, 2) = 10
        ien (5, 3) = 11
        ien (5, 4) = 7
\end{verbatim}

There are three kinds of opperations we might think of wanting.  The first is
taking  values of some function (coordinates, velocities, temperatures,
stresses, etc.) defined on the global grid and getting the values for a single
element, this is called a {\em gather} operation.  The next is taking values in
an element and spreading them out to the global array, this is called a {\em
scatter} operation.  The third operation is to take the value at a local node
and add it to the global value for that node, an {\em assembly} step.

All three operations, gather, scatter, and assemble are done by the routine
local.   Because it is called by the genshp routine above, it is a good example
to look at.   Next class time I will show you how it looks in the code where we
generate the stiffness matrix.

\tt
\begin{verbatim}
      subroutine local (ien, global, rlocal, n, nenloc,  code)
c
c----------------------------------------------------------------------
c
c This subroutine localizes the values via the element group  IEN
c array.  The localize values are spread with the element number as
c its first (fastest) index for vectorization.
c 
c input: 
c  ien   (numel,nen)       : the element connectivity
c  global (n,numnp)        : the global array
c  rlocal (lvec,n,nenl)    : the local array
c  n                       : number of d.o.f's to be copied
c  nenl                    : local number of element nodes
c  code                    : the transfer code
c                              .eq. 'localize', from global to local
c                              .eq. 'globaliz', from local to global
c                              .eq. 'add glob', add  local to global
c  nvec        : vector length, number of elements ( pasted from common)
c  nelvec      : number of elments in group ( pasted from common)
c
c output:
c----------------------------------------------------------------------
c
c
      implicit double precision (a-h,o-z)
c 
c.... deactivate above card(s) for single precision operation 
c
      include 'common.h' 
c
      dimension rlocal(lvec,n,1), global(n,1), ien(numel,1)
      character*8 code 
c 
c.... -------------------->  'localize'  <--------------------
c
      if (code .eq. 'localize') then
c
c.... loop over element nodes, d.o.f's and number of element
c
c$dir scalar
        do 300 ienl = 1, nenloc
c$dir scalar
          do 200 i = 1, n
c$dir no_recurrence
            do 100 iv = 1, nvec
              rlocal(iv,i,ienl) = global(i,ien(iv,ienl))
100         continue
200       continue
300     continue
c
c.... return
c
      return
      endif
c 
c.... -------------------->  'globaliz'  <--------------------
c
      if (code .eq. 'globaliz') then
c
c.... loop over element nodes, d.o.f's and number of element
c 
c$dir scalar
        do 600 ienl = 1, nenloc
c$dir scalar
          do 500 i = 1, n
c$dir no_recurrence
            do 400 iv = 1, nvec
              global(i,ien(iv,ienl)) = rlocal(iv,i,ienl)
400         continue
500       continue
600     continue
c
c.... return
c
        return
      endif
c 
c.... -------------------->  'add glob'  <--------------------
c
      if (code .eq. 'add glob') then
c
c.... loop over element nodes, d.o.f's and number of element
c
c$dir scalar
        do 900 ienl = 1, nenloc
c$dir scalar
          do 800 i = 1, n
c$dir no_recurrence
            do 700 iv = 1, nvec
              global(i,ien(iv,ienl)) = global(i,ien(iv,ienl)) +
     &                                 rlocal(iv,i,ienl)
700         continue
800       continue
900     continue
c
c.... return
c
      return
      endif
c
c.... -------------------->  error  <--------------------
c
      call error ('local   ', code, 0)
c
c.... end
c
      end
\end{verbatim}
\rm

In practice, one would like to separate the gather/scatter and assemble
opperations from the bulk of the computation.  One very important reason for
this is because some computers (vector computers like Crays and parallel
computers) require special thought in how one gathers and scatters information
(because more than one processor may be working on part of the problem at the
same time).   I don't want to add further confusion at this point by diving into
that problem, but that is what ConMan is doing at certain points.   In addition,
you will find as you go through ConMan that I seldom use the routine local. 
Often I put the operations from local directly into the code.   This is because
at one time, it was popular to take any small segment of code and make it a
subroutine.  As the speed of floating point operations (multiplies and adds)
became faster, the overhead associated with calling a subroutine bacame
increasingly expensive compared to the cost of a floating point operation.  Thus
code writing moved away from having many small subrouines (as is the style of
{\bf dlearn,} the code in Hughes' book, and more like ConMan.

\subsection{Assembly}

Boundary conditions and equation numbers (the lm array).

\section{Stiffness Matrix for Viscous Flow: Penalty Method}

There are several methods for solving viscous flow problems with the finite
element method.   'Mixed methods' solve for velocities and pressures.   They
have the drawback that the amount of computation and the amount of storage
increases.  The 'penalty method' is a classical method that approaches the
incompressible limit.  A number of tests with the penalty formulation have
shown that the solutions are in very good agreement with solutions from other
methods, such as stream-function methods which are truely incompressible.  The
penalty method eliminates the pressure from the equations, but there are some
minor problems that result (nothing comes for free).   If you want to look at
the results of some 2-D incompressible codes and see how the penalty method
compares, you should look at the following references.

\begin{quotation}
\noindent 
Travis, B. J., C. Anderson, J. Baumgardner, C. Gable, B. H. Hager, P.
Olson, R. J. O'Connell, A. Raefsky, and G. Schubert, A benchmark comparison of
numerical methods for infinite Prandtl number convection in two-dimensional
Cartesian geometry.  {\em Geophys. Astrophys. Fluid Dyn., 55,} 137-180, 1990. 

\medskip
\noindent
Blankenbach, B., et al., A benchmark comparison for mantle convection codes, 
{\it Geophys. J. Int., 98,} 23-38, 1989. 
\end{quotation} 

At this point, lets return to the 2-D viscous flow equations, written in terms
of stresses, instead of velocities
\begin{eqnarray}  
\tau_{ij,j} + f_i & = & 0 \label{eq:motion}\\
u_{i,i} & = & 0 
\end{eqnarray}
where
\begin{equation}
\tau_{ij} = -p \delta_{ij} + 2 \mu u_{(i,j)} \label{eq:constit}
\end{equation}
where
\begin{equation}
u_{(i,j)} = (u_{i,j} + u_{j,i})/2
\end{equation}
We replace equation~\ref{eq:constit} with the following relationships 
\begin{eqnarray}
\tau_{ij} = -p^\lambda \delta_{ij} + 2 \mu u_{(i,j)} \label{eq:constit-lam} \\
0 = u_{i,i} + p^\lambda/\lambda. \label{eq:constit-lam2}
\end{eqnarray}
As $\lambda$ approaches infinity, these relations approach the incompressible
solution.  Also, as $\lambda$ approaches infinity, $p^\lambda$ approaches the
hydrostatic pressure in the incompressible case.   In general, the hydrostatic
pressure is $-\tau_{ii}/3$.  Substituting Equation~\ref{eq:constit-lam2}
into~\ref{eq:constit-lam} we get 
\begin{equation}
\tau_{ij} = \lambda u_{i,i} \delta_{ij} + 2 \mu u_{(i,j)}
\label{eq:constit-lam-final} 
\end{equation}
or 
\begin{equation}
\tau_{ii} = 3 \lambda u_{i,i} + 2 \mu u_{i,i}
\end{equation}
or
\begin{equation}
\tau_{ii}/3 = -p = (\lambda + 2/3 \mu) u_{i,i}
\end{equation}
but we also have 
\begin{equation}
-p^\lambda = \lambda u_{i,i}  
\end{equation}
from Equation~\ref{eq:constit-lam2}. Clearly in the incompressible limit
$\lambda \gg \mu$ then $\lambda + 2/3 \mu \rightarrow \lambda$ and $p^\lambda
\rightarrow p$.  Also note that the continuity equation is satisfied.

Now, substituting Equation~\ref{eq:constit-lam-final} into
Equation~\ref{eq:motion} we have
\begin{equation}
\{ \lambda u_{i,i} \delta_{ij} + 2 \mu u_{(i,j)} \},j + f_i = 0
\end{equation}
At this point, it is probably easier to switch to differential notation.  I
will also specialize to 2-D:
\begin{eqnarray}
\frac{\partial}{\partial x} \{ 
\lambda ( \frac{\partial u}{\partial x} + \frac{\partial v}{\partial z}) 
+ 2 \mu (\frac{\partial u}{\partial x} + \frac{\partial u}{\partial x} )/2 \}+
\frac{\partial}{\partial z} \{
2 \mu (\frac{\partial v}{\partial x} + \frac{\partial u}{\partial z} )/2 \}+
f_x = 0 \\
\frac{\partial}{\partial x} \{ 
2 \mu (\frac{\partial u}{\partial z} + \frac{\partial v}{\partial x} )/2  \}+
\frac{\partial}{\partial z} \{
\lambda ( \frac{\partial u}{\partial x} + \frac{\partial v}{\partial z} )
+ 2 \mu (\frac{\partial v}{\partial z} + \frac{\partial v}{\partial z} )/2 \}+
f_z = 0
\end{eqnarray}

These are second order partial differential equations.   Simplifying, I get
\begin{eqnarray}
\lambda (
\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 v}{\partial x\partial z} ) 
+ 2 \mu \frac{\partial^2 u}{\partial x^2} 
+ \mu ( \frac{\partial^2 u}{\partial z^2} 
+ \frac{\partial^2 v}{\partial z\partial x} ) 
+ f_x = 0 \\
\lambda ( 
\frac{\partial^2 u}{\partial z\partial x} + \frac{\partial^2 v}{\partial z^2} )
+ \mu ( \frac{\partial^2 u}{\partial x\partial z}
+ \frac{\partial^2 v}{\partial x^2} )
+ 2 \mu \frac{\partial^2 v}{\partial z^2} 
+ f_z = 0
\end{eqnarray}

Now we use the same technique (approach) as we used in Possion's equation to
turn the differential form into an integral form.   You can either look at it
as we find the variational form of the stokes equation (which is what we are
doing) or you can think of it as multiplying by a weighting function $w$ and
integrating over the domain.  Then using integration by parts to convert the
second derivatives to first derivatives.   This is done in carefully by
Hughes on pages 197-200, but he has left out a number of intermediate steps. 
Nothing about this step is hard, it is just tedious.  There is, however, a
cleaver short cut.   If we return to the messy equations at the top of the
page, multiply them by the weighting function $w$ and integrate over the
domain, then we do not have to use integration by parts.   If you are confused,
or don't believe me, then you should take the equations directly above this
paragraph, multiply by a weighting function $w$ and integrate over the 2-D
domain $\Omega$, then use integration by parts.   You will find (after a little
algebra)

\begin{eqnarray}
\int\int _{\Omega} \frac{\partial w}{\partial x} \{ 
\lambda ( \frac{\partial u}{\partial x} + \frac{\partial v}{\partial z})  
+ 2 \mu \frac{\partial u}{\partial x} \}+
\frac{\partial w}{\partial z} \{ 2 \mu (\frac{\partial v}{\partial x} +
\frac{\partial u}{\partial z} )/2 \} \, d\Omega + \nonumber \\
\int\int _{\Omega} f_x \, w \, d\Omega = b.c.~terms \\
\int\int _{\Omega} \frac{\partial w}{\partial x} \{  
2 \mu (\frac{\partial u}{\partial z} + \frac{\partial v}{\partial x} )/2  \}+
\frac{\partial w}{\partial z} \{
\lambda ( \frac{\partial u}{\partial x} + \frac{\partial v}{\partial z} ) + 
2 \mu \frac{\partial v}{\partial z} \} \, d\Omega + \nonumber \\
\int\int _{\Omega} f_z \, w \, d\Omega = b.c.~terms
\end{eqnarray}
Note that we don't get something for nothing, this short cut does not give us
the boundary condition terms (velocity or flux).  These would fall out of the
integration by parts.  Recall,
\begin{equation}
\int _a ^b w \, dv = w\, v | _a ^b - \int _a ^b v \, dw
\end{equation}
were in our case $w$ is the weighting function and $v$ is the second derivative
term.  The first term gives us the flux (first derivative) boundary
conditions.  In the case of the momentum equations, that is the applied
tractions (or stress boundary conditions).

Now we make use of Galerkin's approximation, or more simply, we use the same
weighting functions as we use for interpolation function, i.e., the shape
functions, N.  So we substitute
\begin{eqnarray}
\frac{\partial w}{\partial x} = N_x \\
\frac{\partial w}{\partial z} = N_z \\
\frac{\partial u}{\partial x} = u\, N_x \\
\frac{\partial u}{\partial z} = u\, N_z \\
\frac{\partial v}{\partial x} = v\, N_x \\
\frac{\partial v}{\partial z} = v\, N_z 
\end{eqnarray}
into our weak form equations.   Although messy, that is straight-forward.
\begin{eqnarray}
\int\int _{\Omega} N_x \{ 
\lambda ( u\, N_x + v\, N_z)  + 2 \mu u\, N_x \} +
N_z \{ \mu ( v\, N_x + u\, N_z ) \} \, d\Omega + \nonumber \\
\int\int _{\Omega} f_x \, w \, d\Omega = b.c.~terms \\
\int\int _{\Omega} N_x \{    \mu ( u\, N_z + v\, N_x ) \}+
N_z \{ \lambda ( u\, N_x + v\, N_z ) +  
2 \mu v\, N_z \} \, d\Omega + \nonumber \\
\int\int _{\Omega} f_z \, w \, d\Omega = b.c.~terms
\end{eqnarray}

At this point, it is useful to separate the equations into a $\lambda$ part and
a $\mu$ part.  We can also write them as a 2-D matrix equation
\begin{equation}  [K_{\lambda}] = \left[ 
\begin{array}{cc}
N_x \lambda N_x &  N_x \lambda N_z \\ 
N_z \lambda N_x &  N_z \lambda N_z 
\end{array} \right ] \label{eq:Klambda} 
\end{equation}
and
\begin{equation}  
[K_{\mu}] = \left[ 
\begin{array}{cc} 
N_x 2 \mu N_x + N_z \mu N_z &  N_z \mu N_x \\  
N_x \mu N_z                 &  N_z 2 \mu N_z + N_x \mu N_x
\end{array} \right ]. \label{eq:Kmu} 
\end{equation}

Hughes makes use of an interesting, and important observation.   This
observation will greatly simplify constructing the stiffness matrix for
arbitrary coordinate systems.  We can rewrite the stiffness matrices above in
the following form:
\begin{equation}  
[K_{\lambda}] + [K_{\mu}] = [B]^T [D] [B]
\end{equation}
\begin{equation}
[D_{\lambda}] + [D_{\mu}] = [D]
\end{equation}
where
\begin{equation}   
[D_{\mu}] = \mu \left[ 
\begin{array}{ccc}  
2 & 0 & 0 \\  
0 & 2 & 0 \\
0 & 0 & 1 
\end{array} \right ]
\end{equation}
and
\begin{equation}    
[D_{\lambda}] = \lambda \left[ 
\begin{array}{ccc}   
1 & 1 & 0 \\   
1 & 1 & 0 \\ 
0 & 0 & 0 
\end{array} \right ]
\end{equation}
and
\begin{equation}     
[B] = \left[ 
\begin{array}{cc}    
N_x & 0   \\    
0   & N_z \\
N_z & N_x 
\end{array} \right ].
\end{equation}

Notice what the $[B]$ matrix is...
\begin{equation}
[B] \{\vec u\} = ?
\end{equation}

\subsection{The Element Stiffness Matrix For 2-D Viscous Flow}

Lets return to our 2-D, four node, bi-linear-velocity element.   We want to
form the `element stiffness matrix' for that element.   Then we will assemble
the element stiffness matrices for all the elements together to form the global
stiffness matrix.   Because there are 2 degrees of freedom ($u,v$) for each
node, there will be eight unknowns for each element.   In the same way that 
the 2 by 2 element matricies are symmetric, it will turn out that element
stiffness matrix is symmetric, and infact the global matrix will be symmetric. 
Symmetric matricies are one of the properties of Galerkin formulations, you
can read the details in Hughes.  Because we want to be stingy with memory, we
will only form and store the upper triangular part of the stiffness matrix.  
If we formed these one element at a time, the savings would not be worth the
effort, but because we will form many elements at one time on a vector machine,
we need to be careful.  In ConMan, I treat the 8 by 8 stiffness matrix as a 1-D
array with 36 elements.   The numbering as as follows:
\begin{equation}      
[K^e] = \left[ 
\begin{array}{cccccccc}    
 1 &  2 &  4 &  7 & 11 & 16 & 22 & 29 \\
   &  3 &  5 &  8 & 12 & 17 & 23 & 30 \\
   &    &  6 &  9 & 13 & 18 & 24 & 31 \\
   &    &    & 10 & 14 & 19 & 25 & 32 \\
   &    &    &    & 15 & 20 & 26 & 33 \\
   &    &    &    &    & 21 & 27 & 34 \\
   &    &    &    &    &    & 28 & 35 \\
   &    &    &    &    &    &    & 36 
\end{array} \right ].
\end{equation}
You can think of this 8 by 8 matrix as sixteen 2 by 2 matricies.   Each 2 by 2
matrix is made of elements from the 2 by 2 matrices~\ref{eq:Klambda}
and~\ref{eq:Kmu}.

Recall that the shape functions for the four nodes look like
\begin{eqnarray} 
N_1 & = & \frac{(a-x)(b-y)}{4ab}  \\ 
N_2 & = & \frac{(a+x)(b-y)}{4ab}  \\ 
N_3 & = & \frac{(a+x)(b+y)}{4ab}  \\ 
N_4 & = & \frac{(a-x)(b+y)}{4ab}  
\end{eqnarray}
or in the fortran code, these are stored in the array shl as: shl(1,gp),
shl(2,gp), shl(3,gp), shl(4,gp), where {\em gp} is the number of the gauss 
point.

If we go back to the 2 by 2 matrix for the $\lambda$ part of the stiffness
matrix, Equation~\ref{eq:Kmu}, note that the first shape function in each
matrix element came from the weighting function and the second shape function
came from the interolation function
\begin{equation}  [K_{\lambda}] = \left[ 
\begin{array}{cc} 
N_x(from~w) \lambda N_x(from~u_g) &  N_x(from~w) \lambda N_z(from~v_g) \\  
N_z(from~w) \lambda N_x(from~u_g) &  N_z(from~w) \lambda N_z(from~v_g) 
\end{array} \right ] 
\end{equation}
This helps illustrate how the elements of the 8 by 8 matrix come together.  I'm
going to substitute things in one step at a time, to help you visualize what is
happening.  Lets substitue the fortran arrays in for the shape function
derivatives for the interpolation functions, and the $u_g$'s.
\begin{eqnarray}
N_x(from~u_g) & = & shdx(ivel,i,gp) \\
N_z(from~u_g) & = & shdy(ivel,i,gp) \\
u_g           & = & vl(iv,k), where~k=1,3,5,~or~7 \\
v_g           & = & vl(iv,k), where~k=2,4,6,~or~8
\end{eqnarray}

\begin{displaymath}  
[K^e_{\lambda}] =
\end{displaymath}
{\tiny
\begin{displaymath}\left[ 
\begin{array}{cccc}    
N_x \lambda shdx(ivel,1,gp) & N_x \lambda shdy(ivel,1,gp) & 
N_x \lambda shdx(ivel,2,gp) & N_x \lambda shdy(ivel,2,gp) \\ 

\null                       & N_z \lambda shdy(ivel,1,gp) & 
N_z \lambda shdx(ivel,2,gp) & N_z \lambda shdy(ivel,2,gp) \\ 
 
\null                       & \null                       & 
N_x \lambda shdx(ivel,2,gp) & N_x \lambda shdy(ivel,2,gp) \\ 

\null                       & \null                       & 
\null                       & N_z \lambda shdy(ivel,2,gp) \\ 

\null                       & \null                       & 
\null                       & \null                       \\ 

\null                       & \null                       & 
\null                       & \null                       \\ 

\null                       & \null                       & 
\null                       & \null                       \\ 

\null                       & \null                       & 
\null                       & \null                       \\ 
\end{array} \right .
\end{displaymath} 
\begin{equation} \left .
\begin{array}{cccc}      
N_x \lambda shdx(ivel,3,gp) & N_x \lambda shdy(ivel,3,gp) &  
N_x \lambda shdx(ivel,4,gp) & N_x \lambda shdy(ivel,4,gp) \\ 

N_z \lambda shdx(ivel,3,gp) & N_z \lambda shdy(ivel,3,gp) & 
N_z \lambda shdx(ivel,4,gp) & N_z \lambda shdy(ivel,4,gp) \\ 
 
N_x \lambda shdx(ivel,3,gp) & N_x \lambda shdy(ivel,3,gp) &  
N_x \lambda shdx(ivel,4,gp) & N_x \lambda shdy(ivel,4,gp) \\ 

N_z \lambda shdx(ivel,3,gp) & N_z \lambda shdy(ivel,3,gp) & 
N_z \lambda shdx(ivel,4,gp) & N_z \lambda shdy(ivel,4,gp) \\ 

N_x \lambda shdx(ivel,3,gp) & N_x \lambda shdy(ivel,3,gp) &  
N_x \lambda shdx(ivel,4,gp) & N_x \lambda shdy(ivel,4,gp) \\ 

\null                       & N_z \lambda shdy(ivel,3,gp) & 
N_z \lambda shdx(ivel,4,gp) & N_z \lambda shdy(ivel,4,gp) \\ 

\null                       & \null                       & 
N_x \lambda shdx(ivel,4,gp) & N_x \lambda shdy(ivel,4,gp) \\ 

\null                       & \null                       &
\null                       & N_z \lambda shdy(ivel,4,gp) \\ 
\end{array} \right ].
\end{equation} }

Think of this as the $\lambda$ part of the the element matrix equation
\begin{equation}
[ K^e ] \{ vl \} = \{ f^e \}
\end{equation}
where $\{ vl \}$ and $\{ f^e \}$ are the arrays of local unknowns and local
buoyancy forces.    In the same way that we substituted the shape functions for
the interpolation function, we substitute the shape functions for the weighting
functions (the remaining $N_x, N_z$'s in the matrix above.   I am doing this
step by step so you can see which node gets connected with which node....

\begin{displaymath}   [K^e_{\lambda}] =
\end{displaymath} {\tiny
\begin{displaymath}\left[ 
\begin{array}{cccc}     

shdx(ivel,1,gp) \lambda shdx(ivel,1,gp) & shdx(ivel,1,gp) \lambda
shdy(ivel,1,gp) &   
shdx(ivel,1,gp) \lambda shdx(ivel,2,gp) & shdx(ivel,1,gp) \lambda
shdy(ivel,2,gp) \\ 

\null                       & shdy(ivel,1,gp) \lambda shdy(ivel,1,gp) &  
shdy(ivel,1,gp) \lambda shdx(ivel,2,gp) & shdy(ivel,1,gp) \lambda
shdy(ivel,2,gp) \\ 
 
\null                       & \null                       &  
shdx(ivel,2,gp) \lambda shdx(ivel,2,gp) & shdx(ivel,2,gp) \lambda
shdy(ivel,2,gp) \\ 

\null                       & \null                       & 
\null                       & shdy(ivel,2,gp) \lambda shdy(ivel,2,gp) \\ 

\null                       & \null                       & 
\null                       & \null                       \\ 

\null                       & \null                       & 
\null                       & \null                       \\ 

\null                       & \null                       & 
\null                       & \null                       \\ 

\null                       & \null                       & 
\null                       & \null                       \\ 
\end{array} \right .
\end{displaymath} 
\begin{equation} \left .
\begin{array}{cccc}       
shdx(ivel,1,gp) \lambda shdx(ivel,3,gp) & shdx(ivel,1,gp) \lambda
shdy(ivel,3,gp) &    
shdx(ivel,1,gp) \lambda shdx(ivel,4,gp) & shdx(ivel,1,gp) \lambda
shdy(ivel,4,gp)
\\ 

shdy(ivel,1,gp) \lambda shdx(ivel,3,gp) & shdy(ivel,1,gp) \lambda
shdy(ivel,3,gp) &   
shdy(ivel,1,gp) \lambda shdx(ivel,4,gp) & shdy(ivel,1,gp) \lambda
shdy(ivel,4,gp) \\ 
 
shdx(ivel,2,gp) \lambda shdx(ivel,3,gp) & shdx(ivel,2,gp) \lambda
shdy(ivel,3,gp) &    
shdx(ivel,2,gp) \lambda shdx(ivel,4,gp) & shdx(ivel,2,gp) \lambda
shdy(ivel,4,gp) \\ 

shdy(ivel,2,gp) \lambda shdx(ivel,3,gp) & shdy(ivel,2,gp) \lambda
shdy(ivel,3,gp) &   
shdy(ivel,2,gp) \lambda shdx(ivel,4,gp) & shdy(ivel,2,gp) \lambda
shdy(ivel,4,gp) \\ 

shdx(ivel,3,gp) \lambda shdx(ivel,3,gp) & shdx(ivel,3,gp) \lambda
shdy(ivel,3,gp) &    
shdx(ivel,3,gp) \lambda shdx(ivel,4,gp) & shdx(ivel,3,gp) \lambda
shdy(ivel,4,gp) \\ 

\null                                   & shdy(ivel,3,gp) \lambda 
shdy(ivel,3,gp) &     
shdy(ivel,3,gp) \lambda shdx(ivel,4,gp) & shdy(ivel,3,gp) \lambda
shdy(ivel,4,gp) \\ 

\null                                   & \null                       &  
shdx(ivel,4,gp) \lambda shdx(ivel,4,gp) & shdx(ivel,4,gp) \lambda
shdy(ivel,4,gp) \\ 

\null                       & \null                       &
\null                       & shdy(ivel,4,gp) \lambda shdy(ivel,4,gp) \\ 
\end{array} \right ].
\end{equation} }

Now putting all this together you have enough information to finish your code project.

\end{document}